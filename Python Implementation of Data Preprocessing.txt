# Importing the required librariesimport numpy as np # linear algebraimport pandas as pdfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.metrics import confusion_matrix, classification_reportimport seaborn as snfrom sklearn.model_selection import train_test_splitfrom sklearn import preprocessing, neighbors, svmfrom sklearn.svm import SVCfrom sklearn.naive_bayes import GaussianNBfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifierfrom sklearn.svm import SVC #importing the support vector machinefrom sklearn.naive_bayes import GaussianNBimport matplotlib.pyplot as pltimport seaborn as snimport warningswarnings.filterwarnings('ignore')# Importing the datasetdf = pd.read_csv("BenPoly_Admission.csv")# head viwing of the datasetdf.head(3)# Viewing dataset to determine data items typesdf.info()# Data cleaning by droping unwanted columns df = df.drop(['id', 'firstname','surname','middlename','countryid','dateofbirth', 'homeaddress', 'applicationsession', 'phonenumber', 'sponsorname', 'COURSE NAME', 'LGA', 'STATE', 'sponsoraddress'], axis=1)# viewing to determine data shapedf.shape# Value count determination for gender, marital status, mode of entry, and course categorydf['gender'].value_counts()df['maritalstatus'].value_counts()df['modeofentry'].value_counts()# dropping records with space in mode of entry columndf = df.drop(df[df['modeofentry'] == ' JAMB'].index) #dropping row with space for modeof entry word - JAMBdf['Course Category'].value_counts()df = df.drop(df[df['Course Category'] == ' NA'].index) #dropping row with space for modeof entry word - JAMBdf_cat = df.select_dtypes(object) # selecting columns with object in orde to convert to numeric# Visualizing for all variable with histogramplt.figure()columns = df_cat.columns # creating histogram to visualize the variablesfor col in columns:    print('col: ', col)    df_cat[col].hist()    plt.show()df_num = df.select_dtypes(['float64', 'int64']) # selecting columns with numeric variables# Performing feature engineeringcurrent_qualification = {'Higher National Diploma': 'National diploma', 'National Diploma': 'O-level', 'Pre-ND / Certificate Courses': 'O-level'} # Creat a new diction first containing the values desired in the new featuredf['Current_Qualification'] = df['Course Category'].map(current_qualification) # called the map function pass the diction created earlierdf.head(3)df['Admission_Status'] = df['Admission Status']df = df.drop(['Course Category', 'Admission Status'], axis=1) # dropping old features after feature engineeringdf_cat.head(2) # Converting categorical variables to numbersfor col in df_cat:    df_cat[col] = pd.get_dummies(df_cat[col])# Final dataset for modeldf1 = pd.concat([df_cat, df_num], axis=1)df1.head(2)# Visualizing for all variable with histogram for first dataset df1plt.figure()columns=df1.columns # creating histogram to visualize the variablesfor col in columns:    print('col: ', col)    df1[col].hist()    plt.show()# Removing Age outlier for the first dataset df1plt.figure()sn.boxplot(data=df1, y='Age', x='Admission_Status')plt.show()Under_Aged = df1[(df1['Age']< 15)]df1.drop([146, 285, 376, 1719, 2250, 2601, 3416, 4016, 4089, 4226, 4906, 5137, 5141,12002, 11873, 11448,11381, 11153, 11121, 10657, 9604, 9222, 8983, 8758, 8607, 8343, 8253, 8191, 7973, 7757,5145, 5147, 5153, 5157, 5165, 5338, 5365, 5826, 6221, 7041, 7409, 10298], inplace=True)plt.figure() # Verifying that outlier on the Age variable is removedsn.boxplot(data=df1, y='Age', x='Admission_Status')plt.show()from scipy.stats import skew # Importing skewness libarary to remove skewness from ageplt.figure() # Applying the skewness library on the Ageprint (skew(df1['Age']))sn.distplot(df1['Age'])plt.show()df1['Age'] = np.sqrt(df1['Age'])print (skew(df1['Age']))